{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Competitive Assignment\n",
    "An explanation this assignment could be found in the .pdf explanation document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insturctions\n",
    "# ============\n",
    "\n",
    "\n",
    "# ================================================================================================================\n",
    "# After manually testing different models with different parameters, I have decided to use the MLPClassifier model.\n",
    "# The MLPClassifier had the best results.\n",
    "# ================================================================================================================\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1. Run Imports section.\n",
    "# 2. Run Data Load section.\n",
    "# 3. Run Data Cleansing section.\n",
    "# 4. Run Optimization section - Optimization will take a few minutes to finish.\n",
    "# 5. Run Final Model section.\n",
    "# 6. Run Prediction section.\n",
    "# 7. Run Output section.\n",
    "# =============================\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "# Data Cleansing removes any non-hebrew character from the data.\n",
    "# Optimization searches for the best parameters, testing each combination 5 iterations with shuffled data.\n",
    "# A part of the Optimization was done manually, therefore some parameters are constant to make this section faster.\n",
    "# Final Model section uses the best parameters found in the Optimization section.\n",
    "# The best parameters are based on the average F1 Scores for each N iterations.\n",
    "# The extra model training with the best parameters will be the one to predict the test data.\n",
    "# ================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preceding Step - import modules (packages)\n",
    "This step is necessary in order to use external modules (packages). <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# =======\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input files\n",
    "Reading input files for train annotated corpus (raw text data) corpus and for the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "# =========\n",
    "\n",
    "train_filename = '.' + os.sep + 'input' + os.sep + 'annotated_corpus_for_train.xlsx'\n",
    "test_filename  = '.' + os.sep + 'input' + os.sep + 'corpus_for_test.xlsx'\n",
    "df_train = pd.read_excel(train_filename, 'corpus', index_col=None, na_values=['NA'])\n",
    "df_test  = pd.read_excel(test_filename,  'corpus', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your implementation:\n",
    "Write your code solution in the following code-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing\n",
    "# ==============\n",
    "\n",
    "texts = 'story'\n",
    "label = 'gender'\n",
    "\n",
    "# Remove non-hebrew words & characters\n",
    "df_train[texts] = df_train[texts].str.replace(r'[\\W\\da-zA-Z]+', ' ')\n",
    "df_test[texts] = df_test[texts].str.replace(r'[\\W\\da-zA-Z]+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6949733992203353\n",
      "Highest Average F1_Score So Far =  0.6949733992203353\n",
      "Highest Average F1_Score Parameters: [175, 0]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6897501294531422\n",
      "Highest Average F1_Score So Far =  0.6949733992203353\n",
      "Highest Average F1_Score Parameters: [175, 0]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6963404033266418\n",
      "Highest Average F1_Score So Far =  0.6963404033266418\n",
      "Highest Average F1_Score Parameters: [175, 2]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.642320684655187\n",
      "Highest Average F1_Score So Far =  0.6963404033266418\n",
      "Highest Average F1_Score Parameters: [175, 2]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6170319737299801\n",
      "Highest Average F1_Score So Far =  0.6963404033266418\n",
      "Highest Average F1_Score Parameters: [175, 2]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6575579206571506\n",
      "Highest Average F1_Score So Far =  0.6963404033266418\n",
      "Highest Average F1_Score Parameters: [175, 2]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6506392437718095\n",
      "Highest Average F1_Score So Far =  0.6963404033266418\n",
      "Highest Average F1_Score Parameters: [175, 2]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6538506007441727\n",
      "Highest Average F1_Score So Far =  0.6963404033266418\n",
      "Highest Average F1_Score Parameters: [175, 2]\n",
      "\n",
      "Start iterations\n",
      "End Iterations\n",
      "\n",
      "Average F1_Score For Current Loop =  0.6143210001257002\n",
      "Highest Average F1_Score So Far =  0.6963404033266418\n",
      "Highest Average F1_Score Parameters: [175, 2]\n",
      "\n",
      "\n",
      "Optimiziation Finished\n"
     ]
    }
   ],
   "source": [
    "# Optimization\n",
    "# ============\n",
    "\n",
    "# Parameters for best optimization\n",
    "iter = 5\n",
    "layers = [175, 176, 177]\n",
    "random = [0, 1, 2]\n",
    "params = []\n",
    "params_current = []\n",
    "\n",
    "f1_avg_max = 0\n",
    "\n",
    "for l in layers:\n",
    "    for r in random:\n",
    "        f1_avg = 0\n",
    "        f1_current_max = 0\n",
    "\n",
    "        print()\n",
    "        print(\"Start iterations\")\n",
    "\n",
    "        for i in range(0, iter):\n",
    "\n",
    "            # Scramble the train set\n",
    "            df_train = df_train.sample(frac=1)#.reset_index(drop=True)\n",
    "\n",
    "            # Initialize & use CountVectorizer\n",
    "            vectorizer = CountVectorizer()\n",
    "            train_data = vectorizer.fit_transform(df_train[texts])\n",
    "\n",
    "            # Vectorize the data \n",
    "            X = pd.DataFrame(train_data.toarray(), columns = vectorizer.get_feature_names())\n",
    "            y = df_train[\"gender\"]\n",
    "\n",
    "            # Divide the data into train & validation\n",
    "            training_portion = int(X.shape[0] * 0.7)\n",
    "            X_train = X.iloc[0:training_portion]\n",
    "            y_train = y.iloc[0:training_portion]\n",
    "            X_validation = X.iloc[training_portion:]\n",
    "            y_validation = y.iloc[training_portion:]\n",
    "\n",
    "            # Train the model & Evaluate validation set\n",
    "            clf = MLPClassifier(activation='identity', hidden_layer_sizes=l, random_state=r, tol=0.00001)\n",
    "            clf.fit(X_train, y_train)\n",
    "            pred = clf.predict(X_validation)\n",
    "\n",
    "            # average = macro represents the average F1 Score between 'f' and 'm' labels\n",
    "            f1 = f1_score(y_validation, pred, average='macro') \n",
    "            f1_avg += f1\n",
    "\n",
    "            if f1 > f1_current_max:\n",
    "                f1_current_max = f1\n",
    "                params_current = [l, r]\n",
    "\n",
    "        # Save the parameters based on the best results\n",
    "        f1_avg = f1_avg / iter\n",
    "        if f1_avg > f1_avg_max:\n",
    "            f1_avg_max = f1_avg\n",
    "            params = params_current\n",
    "\n",
    "        print(\"End Iterations\")\n",
    "        print()\n",
    "        print(\"Average F1_Score For Current Loop = \", f1_avg)\n",
    "        print(\"Highest Average F1_Score So Far = \", f1_avg_max)\n",
    "        print(\"Highest Average F1_Score Parameters:\", params)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Optimiziation Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  1\n",
      "Validation F1_Score:\n",
      "0.6468699839486356\n",
      "Average F1_Score Of All Iterations =  0.6468699839486356\n"
     ]
    }
   ],
   "source": [
    "# Final Model Training Using The Best Results Parameters\n",
    "# ======================================================\n",
    "\n",
    "iter = 1\n",
    "f1_avg = 0\n",
    "\n",
    "for i in range(0, iter):\n",
    "    # Scramble the train set\n",
    "    df_train = df_train.sample(frac=1)#.reset_index(drop=True)\n",
    "\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_data = vectorizer.fit_transform(df_train[texts])\n",
    "\n",
    "    # Vectorize the data \n",
    "    X = pd.DataFrame(train_data.toarray(), columns = vectorizer.get_feature_names())\n",
    "    y = df_train[\"gender\"]\n",
    "\n",
    "    # Divide the data into train & validation\n",
    "    training_portion = int(X.shape[0] * 0.7)\n",
    "    X_train = X.iloc[0:training_portion]\n",
    "    y_train = y.iloc[0:training_portion]\n",
    "    X_validation = X.iloc[training_portion:]\n",
    "    y_validation = y.iloc[training_portion:]\n",
    "\n",
    "    # Train the model with the best parameters found at optimization & Evaluate validation set\n",
    "    clf = MLPClassifier(activation='identity', hidden_layer_sizes=params[0], random_state=params[1], tol=0.00001, verbose=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_validation)\n",
    "    \n",
    "    \n",
    "    # average = macro represents the average F1 Score between 'f' and 'm' labels\n",
    "    print()\n",
    "    print(\"Iteration \", i+1)\n",
    "    f1 = f1_score(y_validation, pred, average='macro')\n",
    "    f1_avg += f1\n",
    "    print(\"Validation F1_Score:\")\n",
    "    print(f1)\n",
    "\n",
    "f1_avg = f1_avg / iter\n",
    "print(\"Average F1_Score Of All Iterations = \", f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# ==========\n",
    "\n",
    "# Vectorize prediction data\n",
    "test_data = vectorizer.transform(df_test[texts])\n",
    "X_to_predict = pd.DataFrame(test_data.toarray(), columns = vectorizer.get_feature_names())\n",
    "\n",
    "pred = clf.predict(X_to_predict)\n",
    "df_predicted = pd.DataFrame({\"test_example_id\": range(0, pred.shape[0]), \"predicted_category\": pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output to csv\n",
    "After you're done save your output to the 'classification_results.csv' csv file.<br/>\n",
    "We assume that the dataframe with your results contain the following columns:\n",
    "* column 1 (left column): 'test_example_id'  - the same id associated to each of the test stories to be predicted.\n",
    "* column 2 (right column): 'predicted_category' - the predicted gender value for each of the associated story. \n",
    "\n",
    "Assuming your predicted values are in the `df_predicted` dataframe, you should save you're results as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "# ======\n",
    "\n",
    "df_predicted.to_csv('classification_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
